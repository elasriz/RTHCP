---
title: RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms
---

# ğŸ§  Real-Time Hybrid Control with Physics

**Presented at [IROS 2025]**  
[[ğŸ“„ Paper](https://drive.google.com/file/d/1o7KSa_mcdQYd44YF6MeNFWr5cQxrvxb7/view?usp=sharing)] Â· [[ğŸ’» Code](https://github.com/elasriz/RTHCP/)] Â· [[ğŸ¥ Video](https://youtu.be/Janb7beQVwk)]

---

##  ğŸ§  Motivation


Reinforcement Learning (RL) has shown great success in simulated environments, but faces two major bottlenecks in real-world robotic applications:

- **Sample Inefficiency**: Model-free RL requires a large amount of interactions with the environment, which is impractical or costly on physical robots.
- **Inference Delays**: In model-based RL, computing the next action using forward models or planning methods often takes too long to maintain real-time control on embedded hardware.

---

## âš™ï¸ Key Contributions

- **D-step MPC Framework**: A novel method to handle **inference delays** in model-based reinforcement learning by computing and queuing several actions in advance.
- **RT-HCP**: A **real-time hybrid control** policy that combines:
  - model-based planning,
  - model-free actor-critic agent for fast feedback,
  - and prior dynamics models,

---

## ğŸ“ Method Overview

Our pipeline introduces:
- A **Delay Wrapper** that explicitly models and handles inference delays using action and state buffers.
- An **augmented state** formulation that incorporates both current and hidden past states for delay-aware control.
- A **D-step MPC strategy** that precomputes sequences of actions to bridge inference gaps.

<p align="center">
  <img src="media/RTHCP_method.gif" alt="RT-HCP Diagram" width="600"/>
</p>

---

## ğŸ“Š Experimental Results

RT-HCP was evaluated on a real Furuta pendulum system, and compared to several baselines:

| Method     | Performance | Real-time capability | Sample efficiency| 
|------------|-------------|----------------------|------------------|
| RT-HCP     | ğŸŸ¢ High     | âœ… Yes                | ğŸŸ¢ High          |
| RT-TDMPC   | ğŸŸ  Moderate | âœ… Yes                | ğŸŸ  Moderate      |
| RT-PETS    | âš ï¸ Low      | âš ï¸ No                 | ğŸŸ  Moderate      |
| TD3        | ğŸŸ  Moderate | âœ… Yes                | âš ï¸ Very Low      |

RT-HCP enables continuous control and robust policy learning, even under strict real-time constraints.

ğŸ¥ **Watch the policy behaviors:** [Link to video](#)

---

## ğŸ”— Resources

- ğŸ“„ **Paper**: [PDF](#)
- ğŸ’» **Code**: [GitHub Repository](#)
- ğŸ“ **BibTeX**:
```bibtex
@inproceedings{...}
